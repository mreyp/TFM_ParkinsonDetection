{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carpeta 'TFM_MartaRey/ResNet/results_parkinson_real' creada.\n",
      "Method:  resnet\n",
      "2024-05-31 17:21:21.579794: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-31 17:21:21.916607: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-31 17:21:21.916815: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-31 17:21:21.971089: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-31 17:21:22.105353: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-31 17:21:25.369831: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-05-31 17:21:30.711669: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38374 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:86:00.0, compute capability: 8.0\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 11520, 1)]           0         []                            \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)             (None, 11520, 64)            576       ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 11520, 64)            256       ['conv1d[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation (Activation)     (None, 11520, 64)            0         ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)           (None, 11520, 64)            20544     ['activation[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 11520, 64)            256       ['conv1d_1[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_1 (Activation)   (None, 11520, 64)            0         ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)           (None, 11520, 64)            128       ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)           (None, 11520, 64)            12352     ['activation_1[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, 11520, 64)            256       ['conv1d_3[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 11520, 64)            256       ['conv1d_2[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 11520, 64)            0         ['batch_normalization_3[0][0]'\n",
      "                                                                    , 'batch_normalization_2[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_2 (Activation)   (None, 11520, 64)            0         ['add[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)           (None, 11520, 128)           65664     ['activation_2[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, 11520, 128)           512       ['conv1d_4[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_3 (Activation)   (None, 11520, 128)           0         ['batch_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)           (None, 11520, 128)           82048     ['activation_3[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, 11520, 128)           512       ['conv1d_5[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_4 (Activation)   (None, 11520, 128)           0         ['batch_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)           (None, 11520, 128)           8320      ['activation_2[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)           (None, 11520, 128)           49280     ['activation_4[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_7 (Bat  (None, 11520, 128)           512       ['conv1d_7[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_6 (Bat  (None, 11520, 128)           512       ['conv1d_6[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, 11520, 128)           0         ['batch_normalization_7[0][0]'\n",
      "                                                                    , 'batch_normalization_6[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_5 (Activation)   (None, 11520, 128)           0         ['add_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_8 (Conv1D)           (None, 11520, 128)           131200    ['activation_5[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_8 (Bat  (None, 11520, 128)           512       ['conv1d_8[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_6 (Activation)   (None, 11520, 128)           0         ['batch_normalization_8[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_9 (Conv1D)           (None, 11520, 128)           82048     ['activation_6[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_9 (Bat  (None, 11520, 128)           512       ['conv1d_9[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_7 (Activation)   (None, 11520, 128)           0         ['batch_normalization_9[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_10 (Conv1D)          (None, 11520, 128)           49280     ['activation_7[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_11 (Ba  (None, 11520, 128)           512       ['activation_5[0][0]']        \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_10 (Ba  (None, 11520, 128)           512       ['conv1d_10[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_2 (Add)                 (None, 11520, 128)           0         ['batch_normalization_11[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'batch_normalization_10[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_8 (Activation)   (None, 11520, 128)           0         ['add_2[0][0]']               \n",
      "                                                                                                  \n",
      " global_average_pooling1d (  (None, 128)                  0         ['activation_8[0][0]']        \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 2)                    258       ['global_average_pooling1d[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 506818 (1.93 MB)\n",
      "Trainable params: 504258 (1.92 MB)\n",
      "Non-trainable params: 2560 (10.00 KB)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/500\n",
      "2024-05-31 17:21:35.022544: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-05-31 17:21:35.308933: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-05-31 17:21:36.039313: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-05-31 17:21:37.396192: I external/local_xla/xla/service/service.cc:168] XLA service 0x15428a9af440 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-05-31 17:21:37.396226: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-PCIE-40GB, Compute Capability 8.0\n",
      "2024-05-31 17:21:37.434046: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717176097.624107 1939418 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7383 - accuracy: 0.6278\n",
      "Epoch 1: val_loss improved from inf to 0.69360, saving model to TFM_MartaRey/ResNet/results_parkinson_real/results/resnet/dataset_real/best_model.hdf5\n",
      "/home/v890/v890699/miniconda3/envs/TFM/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n",
      "10/10 [==============================] - 14s 308ms/step - loss: 0.7383 - accuracy: 0.6278 - val_loss: 0.6936 - val_accuracy: 0.4667 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5293 - accuracy: 0.7333\n",
      "Epoch 2: val_loss did not improve from 0.69360\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5293 - accuracy: 0.7333 - val_loss: 0.7001 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4934 - accuracy: 0.7333\n",
      "Epoch 3: val_loss did not improve from 0.69360\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4934 - accuracy: 0.7333 - val_loss: 0.7267 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4841 - accuracy: 0.7444\n",
      "Epoch 4: val_loss did not improve from 0.69360\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4841 - accuracy: 0.7444 - val_loss: 0.7768 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4947 - accuracy: 0.7556\n",
      "Epoch 5: val_loss did not improve from 0.69360\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.4947 - accuracy: 0.7556 - val_loss: 0.8186 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5008 - accuracy: 0.7389\n",
      "Epoch 6: val_loss did not improve from 0.69360\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.5008 - accuracy: 0.7389 - val_loss: 0.9098 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4847 - accuracy: 0.7667\n",
      "Epoch 7: val_loss did not improve from 0.69360\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4847 - accuracy: 0.7667 - val_loss: 0.9603 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5350 - accuracy: 0.6833\n",
      "Epoch 8: val_loss did not improve from 0.69360\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5350 - accuracy: 0.6833 - val_loss: 1.0466 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4964 - accuracy: 0.7722\n",
      "Epoch 9: val_loss did not improve from 0.69360\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4964 - accuracy: 0.7722 - val_loss: 0.9929 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4702 - accuracy: 0.7833\n",
      "Epoch 10: val_loss did not improve from 0.69360\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4702 - accuracy: 0.7833 - val_loss: 0.9646 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5230 - accuracy: 0.7111\n",
      "Epoch 11: val_loss did not improve from 0.69360\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5230 - accuracy: 0.7111 - val_loss: 0.8037 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4709 - accuracy: 0.7611\n",
      "Epoch 12: val_loss did not improve from 0.69360\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.4709 - accuracy: 0.7611 - val_loss: 0.9281 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4852 - accuracy: 0.7611\n",
      "Epoch 13: val_loss did not improve from 0.69360\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4852 - accuracy: 0.7611 - val_loss: 0.9467 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4834 - accuracy: 0.7611\n",
      "Epoch 14: val_loss did not improve from 0.69360\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4834 - accuracy: 0.7611 - val_loss: 1.1179 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4449 - accuracy: 0.7833\n",
      "Epoch 15: val_loss did not improve from 0.69360\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4449 - accuracy: 0.7833 - val_loss: 1.1186 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4189 - accuracy: 0.8056\n",
      "Epoch 16: val_loss did not improve from 0.69360\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4189 - accuracy: 0.8056 - val_loss: 1.2886 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 17/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4046 - accuracy: 0.8000\n",
      "Epoch 17: val_loss did not improve from 0.69360\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.4046 - accuracy: 0.8000 - val_loss: 1.4943 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 18/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4479 - accuracy: 0.8222\n",
      "Epoch 18: val_loss did not improve from 0.69360\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4479 - accuracy: 0.8222 - val_loss: 1.5486 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 19/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4675 - accuracy: 0.7611\n",
      "Epoch 19: val_loss did not improve from 0.69360\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4675 - accuracy: 0.7611 - val_loss: 1.0464 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 20/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3865 - accuracy: 0.8278\n",
      "Epoch 20: val_loss did not improve from 0.69360\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.3865 - accuracy: 0.8278 - val_loss: 1.3749 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 21/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4768 - accuracy: 0.7333\n",
      "Epoch 21: val_loss did not improve from 0.69360\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4768 - accuracy: 0.7333 - val_loss: 1.9142 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 22/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5465 - accuracy: 0.7778\n",
      "Epoch 22: val_loss did not improve from 0.69360\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5465 - accuracy: 0.7778 - val_loss: 1.0376 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 23/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4743 - accuracy: 0.7944\n",
      "Epoch 23: val_loss did not improve from 0.69360\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4743 - accuracy: 0.7944 - val_loss: 0.9457 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 24/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4242 - accuracy: 0.8167\n",
      "Epoch 24: val_loss did not improve from 0.69360\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4242 - accuracy: 0.8167 - val_loss: 0.8956 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 25/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4298 - accuracy: 0.7778\n",
      "Epoch 25: val_loss did not improve from 0.69360\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4298 - accuracy: 0.7778 - val_loss: 1.2144 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 26/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4185 - accuracy: 0.7889\n",
      "Epoch 26: val_loss did not improve from 0.69360\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4185 - accuracy: 0.7889 - val_loss: 1.4980 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 27/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4258 - accuracy: 0.8000\n",
      "Epoch 27: val_loss did not improve from 0.69360\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4258 - accuracy: 0.8000 - val_loss: 1.5686 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 28/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4660 - accuracy: 0.7889\n",
      "Epoch 28: val_loss did not improve from 0.69360\n",
      "10/10 [==============================] - 1s 71ms/step - loss: 0.4660 - accuracy: 0.7889 - val_loss: 1.8292 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 29/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4552 - accuracy: 0.7722\n",
      "Epoch 29: val_loss did not improve from 0.69360\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4552 - accuracy: 0.7722 - val_loss: 2.0271 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 30/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3793 - accuracy: 0.8611\n",
      "Epoch 30: val_loss did not improve from 0.69360\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.3793 - accuracy: 0.8611 - val_loss: 1.6612 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 31/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4002 - accuracy: 0.8000\n",
      "Epoch 31: val_loss did not improve from 0.69360\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4002 - accuracy: 0.8000 - val_loss: 1.2853 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 32/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3469 - accuracy: 0.8444\n",
      "Epoch 32: val_loss did not improve from 0.69360\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.3469 - accuracy: 0.8444 - val_loss: 2.5648 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 33/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4055 - accuracy: 0.7889\n",
      "Epoch 33: val_loss did not improve from 0.69360\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4055 - accuracy: 0.7889 - val_loss: 2.7233 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 34/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3677 - accuracy: 0.8556\n",
      "Epoch 34: val_loss did not improve from 0.69360\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.3677 - accuracy: 0.8556 - val_loss: 2.7782 - val_accuracy: 0.5167 - lr: 0.0010\n",
      "Epoch 35/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3637 - accuracy: 0.8444\n",
      "Epoch 35: val_loss did not improve from 0.69360\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.3637 - accuracy: 0.8444 - val_loss: 3.2183 - val_accuracy: 0.5167 - lr: 0.0010\n",
      "Epoch 36/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3365 - accuracy: 0.8667\n",
      "Epoch 36: val_loss did not improve from 0.69360\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.3365 - accuracy: 0.8667 - val_loss: 3.4406 - val_accuracy: 0.5167 - lr: 0.0010\n",
      "Epoch 37/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3917 - accuracy: 0.8167\n",
      "Epoch 37: val_loss did not improve from 0.69360\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.3917 - accuracy: 0.8167 - val_loss: 3.3031 - val_accuracy: 0.5167 - lr: 0.0010\n",
      "Epoch 38/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3549 - accuracy: 0.8111\n",
      "Epoch 38: val_loss did not improve from 0.69360\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.3549 - accuracy: 0.8111 - val_loss: 2.2695 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 39/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4213 - accuracy: 0.8056\n",
      "Epoch 39: val_loss did not improve from 0.69360\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4213 - accuracy: 0.8056 - val_loss: 2.1981 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 40/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3423 - accuracy: 0.8556\n",
      "Epoch 40: val_loss did not improve from 0.69360\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.3423 - accuracy: 0.8556 - val_loss: 1.2573 - val_accuracy: 0.5167 - lr: 0.0010\n",
      "Epoch 41/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3115 - accuracy: 0.8889\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.69360\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.3115 - accuracy: 0.8889 - val_loss: 2.8632 - val_accuracy: 0.5167 - lr: 0.0010\n",
      "Epoch 42/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3144 - accuracy: 0.8556\n",
      "Epoch 42: val_loss did not improve from 0.69360\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.3144 - accuracy: 0.8556 - val_loss: 2.2206 - val_accuracy: 0.5167 - lr: 5.0000e-04\n",
      "Epoch 43/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2772 - accuracy: 0.8833\n",
      "Epoch 43: val_loss did not improve from 0.69360\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.2772 - accuracy: 0.8833 - val_loss: 2.0385 - val_accuracy: 0.5167 - lr: 5.0000e-04\n",
      "Epoch 44/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3128 - accuracy: 0.8611\n",
      "Epoch 44: val_loss did not improve from 0.69360\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.3128 - accuracy: 0.8611 - val_loss: 1.8818 - val_accuracy: 0.5167 - lr: 5.0000e-04\n",
      "Epoch 45/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2601 - accuracy: 0.8889\n",
      "Epoch 45: val_loss did not improve from 0.69360\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.2601 - accuracy: 0.8889 - val_loss: 2.2405 - val_accuracy: 0.5167 - lr: 5.0000e-04\n",
      "Epoch 46/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2425 - accuracy: 0.9111\n",
      "Epoch 46: val_loss did not improve from 0.69360\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.2425 - accuracy: 0.9111 - val_loss: 2.3343 - val_accuracy: 0.5167 - lr: 5.0000e-04\n",
      "Epoch 47/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2454 - accuracy: 0.8889\n",
      "Epoch 47: val_loss did not improve from 0.69360\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.2454 - accuracy: 0.8889 - val_loss: 2.3348 - val_accuracy: 0.5167 - lr: 5.0000e-04\n",
      "Epoch 48/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2214 - accuracy: 0.9278\n",
      "Epoch 48: val_loss did not improve from 0.69360\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.2214 - accuracy: 0.9278 - val_loss: 2.0882 - val_accuracy: 0.5167 - lr: 5.0000e-04\n",
      "Epoch 49/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2264 - accuracy: 0.8944\n",
      "Epoch 49: val_loss did not improve from 0.69360\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.2264 - accuracy: 0.8944 - val_loss: 4.2251 - val_accuracy: 0.5167 - lr: 5.0000e-04\n",
      "Epoch 50/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2857 - accuracy: 0.8778\n",
      "Epoch 50: val_loss did not improve from 0.69360\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.2857 - accuracy: 0.8778 - val_loss: 2.8695 - val_accuracy: 0.5167 - lr: 5.0000e-04\n",
      "Epoch 51/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2494 - accuracy: 0.8889\n",
      "Epoch 51: val_loss did not improve from 0.69360\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.2494 - accuracy: 0.8889 - val_loss: 3.6854 - val_accuracy: 0.5167 - lr: 5.0000e-04\n",
      "Epoch 51: early stopping\n",
      "2/2 [==============================] - 4s 2s/step\n",
      "2/2 [==============================] - 0s 22ms/step\n",
      "VALIDATION METRICS: \n",
      "    accuracy  precision  recall   f1       AUC   duration\n",
      "0  0.466667        0.0     0.0  0.0  0.466667  49.797904\n",
      "TEST METRICS: \n",
      "    accuracy  precision  recall   f1   AUC   duration\n",
      "0      0.45        0.0     0.0  0.0  0.45  49.797904\n",
      "Fold 1 processing complete.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 11520, 1)]           0         []                            \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)             (None, 11520, 64)            576       ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 11520, 64)            256       ['conv1d[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation (Activation)     (None, 11520, 64)            0         ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)           (None, 11520, 64)            20544     ['activation[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 11520, 64)            256       ['conv1d_1[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_1 (Activation)   (None, 11520, 64)            0         ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)           (None, 11520, 64)            128       ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)           (None, 11520, 64)            12352     ['activation_1[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, 11520, 64)            256       ['conv1d_3[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 11520, 64)            256       ['conv1d_2[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 11520, 64)            0         ['batch_normalization_3[0][0]'\n",
      "                                                                    , 'batch_normalization_2[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_2 (Activation)   (None, 11520, 64)            0         ['add[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)           (None, 11520, 128)           65664     ['activation_2[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, 11520, 128)           512       ['conv1d_4[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_3 (Activation)   (None, 11520, 128)           0         ['batch_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)           (None, 11520, 128)           82048     ['activation_3[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, 11520, 128)           512       ['conv1d_5[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_4 (Activation)   (None, 11520, 128)           0         ['batch_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)           (None, 11520, 128)           8320      ['activation_2[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)           (None, 11520, 128)           49280     ['activation_4[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_7 (Bat  (None, 11520, 128)           512       ['conv1d_7[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_6 (Bat  (None, 11520, 128)           512       ['conv1d_6[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, 11520, 128)           0         ['batch_normalization_7[0][0]'\n",
      "                                                                    , 'batch_normalization_6[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_5 (Activation)   (None, 11520, 128)           0         ['add_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_8 (Conv1D)           (None, 11520, 128)           131200    ['activation_5[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_8 (Bat  (None, 11520, 128)           512       ['conv1d_8[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_6 (Activation)   (None, 11520, 128)           0         ['batch_normalization_8[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_9 (Conv1D)           (None, 11520, 128)           82048     ['activation_6[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_9 (Bat  (None, 11520, 128)           512       ['conv1d_9[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_7 (Activation)   (None, 11520, 128)           0         ['batch_normalization_9[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_10 (Conv1D)          (None, 11520, 128)           49280     ['activation_7[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_11 (Ba  (None, 11520, 128)           512       ['activation_5[0][0]']        \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_10 (Ba  (None, 11520, 128)           512       ['conv1d_10[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_2 (Add)                 (None, 11520, 128)           0         ['batch_normalization_11[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'batch_normalization_10[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_8 (Activation)   (None, 11520, 128)           0         ['add_2[0][0]']               \n",
      "                                                                                                  \n",
      " global_average_pooling1d (  (None, 128)                  0         ['activation_8[0][0]']        \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 2)                    258       ['global_average_pooling1d[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 506818 (1.93 MB)\n",
      "Trainable params: 504258 (1.92 MB)\n",
      "Non-trainable params: 2560 (10.00 KB)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6421 - accuracy: 0.6667\n",
      "Epoch 1: val_loss improved from inf to 0.69345, saving model to TFM_MartaRey/ResNet/results_parkinson_real/results/resnet/dataset_real/best_model.hdf5\n",
      "/home/v890/v890699/miniconda3/envs/TFM/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n",
      "10/10 [==============================] - 7s 131ms/step - loss: 0.6421 - accuracy: 0.6667 - val_loss: 0.6935 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6025 - accuracy: 0.6500\n",
      "Epoch 2: val_loss did not improve from 0.69345\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.6025 - accuracy: 0.6500 - val_loss: 0.7026 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5327 - accuracy: 0.7167\n",
      "Epoch 3: val_loss did not improve from 0.69345\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5327 - accuracy: 0.7167 - val_loss: 0.7065 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5516 - accuracy: 0.6667\n",
      "Epoch 4: val_loss did not improve from 0.69345\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5516 - accuracy: 0.6667 - val_loss: 0.7181 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5105 - accuracy: 0.7222\n",
      "Epoch 5: val_loss did not improve from 0.69345\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5105 - accuracy: 0.7222 - val_loss: 0.7355 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5169 - accuracy: 0.7278\n",
      "Epoch 6: val_loss did not improve from 0.69345\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5169 - accuracy: 0.7278 - val_loss: 0.7819 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5068 - accuracy: 0.7222\n",
      "Epoch 7: val_loss did not improve from 0.69345\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5068 - accuracy: 0.7222 - val_loss: 0.7905 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4949 - accuracy: 0.7167\n",
      "Epoch 8: val_loss did not improve from 0.69345\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4949 - accuracy: 0.7167 - val_loss: 0.8069 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5080 - accuracy: 0.7111\n",
      "Epoch 9: val_loss did not improve from 0.69345\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5080 - accuracy: 0.7111 - val_loss: 0.8629 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5063 - accuracy: 0.7444\n",
      "Epoch 10: val_loss did not improve from 0.69345\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5063 - accuracy: 0.7444 - val_loss: 0.8400 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4637 - accuracy: 0.7389\n",
      "Epoch 11: val_loss did not improve from 0.69345\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4637 - accuracy: 0.7389 - val_loss: 0.9450 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5041 - accuracy: 0.7167\n",
      "Epoch 12: val_loss did not improve from 0.69345\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5041 - accuracy: 0.7167 - val_loss: 1.0352 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4464 - accuracy: 0.7667\n",
      "Epoch 13: val_loss did not improve from 0.69345\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4464 - accuracy: 0.7667 - val_loss: 1.1664 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5018 - accuracy: 0.7389\n",
      "Epoch 14: val_loss did not improve from 0.69345\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5018 - accuracy: 0.7389 - val_loss: 0.9963 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4565 - accuracy: 0.7611\n",
      "Epoch 15: val_loss did not improve from 0.69345\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4565 - accuracy: 0.7611 - val_loss: 1.4914 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4582 - accuracy: 0.7944\n",
      "Epoch 16: val_loss did not improve from 0.69345\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4582 - accuracy: 0.7944 - val_loss: 1.2848 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 17/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4429 - accuracy: 0.7722\n",
      "Epoch 17: val_loss did not improve from 0.69345\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4429 - accuracy: 0.7722 - val_loss: 1.5373 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 18/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4215 - accuracy: 0.8000\n",
      "Epoch 18: val_loss did not improve from 0.69345\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4215 - accuracy: 0.8000 - val_loss: 1.2347 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 19/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4413 - accuracy: 0.7833\n",
      "Epoch 19: val_loss did not improve from 0.69345\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4413 - accuracy: 0.7833 - val_loss: 1.1583 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 20/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4813 - accuracy: 0.7278\n",
      "Epoch 20: val_loss did not improve from 0.69345\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4813 - accuracy: 0.7278 - val_loss: 1.1111 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 21/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4229 - accuracy: 0.8222\n",
      "Epoch 21: val_loss did not improve from 0.69345\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4229 - accuracy: 0.8222 - val_loss: 1.2001 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 22/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4091 - accuracy: 0.7611\n",
      "Epoch 22: val_loss did not improve from 0.69345\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4091 - accuracy: 0.7611 - val_loss: 0.9689 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 23/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3916 - accuracy: 0.8111\n",
      "Epoch 23: val_loss did not improve from 0.69345\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.3916 - accuracy: 0.8111 - val_loss: 0.8766 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 24/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3614 - accuracy: 0.8500\n",
      "Epoch 24: val_loss did not improve from 0.69345\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.3614 - accuracy: 0.8500 - val_loss: 1.2029 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 25/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4036 - accuracy: 0.7944\n",
      "Epoch 25: val_loss did not improve from 0.69345\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4036 - accuracy: 0.7944 - val_loss: 0.7484 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 26/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4123 - accuracy: 0.7778\n",
      "Epoch 26: val_loss did not improve from 0.69345\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4123 - accuracy: 0.7778 - val_loss: 0.7501 - val_accuracy: 0.5833 - lr: 0.0010\n",
      "Epoch 27/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3827 - accuracy: 0.8278\n",
      "Epoch 27: val_loss did not improve from 0.69345\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.3827 - accuracy: 0.8278 - val_loss: 1.5927 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 28/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4449 - accuracy: 0.7778\n",
      "Epoch 28: val_loss did not improve from 0.69345\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4449 - accuracy: 0.7778 - val_loss: 2.6015 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 29/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3984 - accuracy: 0.8056\n",
      "Epoch 29: val_loss did not improve from 0.69345\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.3984 - accuracy: 0.8056 - val_loss: 1.7711 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 30/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4446 - accuracy: 0.7722\n",
      "Epoch 30: val_loss did not improve from 0.69345\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.4446 - accuracy: 0.7722 - val_loss: 1.5385 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 31/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3726 - accuracy: 0.8444\n",
      "Epoch 31: val_loss did not improve from 0.69345\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.3726 - accuracy: 0.8444 - val_loss: 1.3708 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 32/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4173 - accuracy: 0.8056\n",
      "Epoch 32: val_loss did not improve from 0.69345\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4173 - accuracy: 0.8056 - val_loss: 1.1245 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 33/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3358 - accuracy: 0.8444\n",
      "Epoch 33: val_loss did not improve from 0.69345\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.3358 - accuracy: 0.8444 - val_loss: 1.1741 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 34/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3698 - accuracy: 0.8167\n",
      "Epoch 34: val_loss did not improve from 0.69345\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.3698 - accuracy: 0.8167 - val_loss: 1.3510 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 35/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3282 - accuracy: 0.8500\n",
      "Epoch 35: val_loss did not improve from 0.69345\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.3282 - accuracy: 0.8500 - val_loss: 1.8101 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 36/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3535 - accuracy: 0.8556\n",
      "Epoch 36: val_loss did not improve from 0.69345\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.3535 - accuracy: 0.8556 - val_loss: 1.0801 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 37/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3215 - accuracy: 0.8722\n",
      "Epoch 37: val_loss did not improve from 0.69345\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.3215 - accuracy: 0.8722 - val_loss: 0.9441 - val_accuracy: 0.6167 - lr: 0.0010\n",
      "Epoch 38/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4016 - accuracy: 0.8056\n",
      "Epoch 38: val_loss did not improve from 0.69345\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4016 - accuracy: 0.8056 - val_loss: 1.8371 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 39/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3369 - accuracy: 0.8611\n",
      "Epoch 39: val_loss did not improve from 0.69345\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.3369 - accuracy: 0.8611 - val_loss: 0.9254 - val_accuracy: 0.6167 - lr: 0.0010\n",
      "Epoch 40/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3656 - accuracy: 0.8167\n",
      "Epoch 40: val_loss did not improve from 0.69345\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.3656 - accuracy: 0.8167 - val_loss: 0.8752 - val_accuracy: 0.5833 - lr: 0.0010\n",
      "Epoch 41/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3283 - accuracy: 0.8556\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.69345\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.3283 - accuracy: 0.8556 - val_loss: 0.9757 - val_accuracy: 0.6667 - lr: 0.0010\n",
      "Epoch 42/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2909 - accuracy: 0.8722\n",
      "Epoch 42: val_loss did not improve from 0.69345\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.2909 - accuracy: 0.8722 - val_loss: 1.4073 - val_accuracy: 0.5500 - lr: 5.0000e-04\n",
      "Epoch 43/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3174 - accuracy: 0.8500\n",
      "Epoch 43: val_loss did not improve from 0.69345\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.3174 - accuracy: 0.8500 - val_loss: 2.6071 - val_accuracy: 0.5000 - lr: 5.0000e-04\n",
      "Epoch 44/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2417 - accuracy: 0.9056\n",
      "Epoch 44: val_loss did not improve from 0.69345\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.2417 - accuracy: 0.9056 - val_loss: 3.5417 - val_accuracy: 0.5000 - lr: 5.0000e-04\n",
      "Epoch 45/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2403 - accuracy: 0.9000\n",
      "Epoch 45: val_loss did not improve from 0.69345\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.2403 - accuracy: 0.9000 - val_loss: 3.5530 - val_accuracy: 0.5000 - lr: 5.0000e-04\n",
      "Epoch 46/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2517 - accuracy: 0.8944\n",
      "Epoch 46: val_loss did not improve from 0.69345\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.2517 - accuracy: 0.8944 - val_loss: 2.7635 - val_accuracy: 0.5000 - lr: 5.0000e-04\n",
      "Epoch 47/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2218 - accuracy: 0.9056\n",
      "Epoch 47: val_loss did not improve from 0.69345\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.2218 - accuracy: 0.9056 - val_loss: 2.3737 - val_accuracy: 0.5000 - lr: 5.0000e-04\n",
      "Epoch 48/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1906 - accuracy: 0.9389\n",
      "Epoch 48: val_loss did not improve from 0.69345\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.1906 - accuracy: 0.9389 - val_loss: 2.4523 - val_accuracy: 0.5000 - lr: 5.0000e-04\n",
      "Epoch 49/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2024 - accuracy: 0.9333\n",
      "Epoch 49: val_loss did not improve from 0.69345\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.2024 - accuracy: 0.9333 - val_loss: 2.3448 - val_accuracy: 0.5000 - lr: 5.0000e-04\n",
      "Epoch 50/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2289 - accuracy: 0.9000\n",
      "Epoch 50: val_loss did not improve from 0.69345\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.2289 - accuracy: 0.9000 - val_loss: 1.6537 - val_accuracy: 0.5000 - lr: 5.0000e-04\n",
      "Epoch 51/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2776 - accuracy: 0.9000\n",
      "Epoch 51: val_loss did not improve from 0.69345\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.2776 - accuracy: 0.9000 - val_loss: 1.2248 - val_accuracy: 0.5500 - lr: 5.0000e-04\n",
      "Epoch 51: early stopping\n",
      "2/2 [==============================] - 0s 23ms/step\n",
      "2/2 [==============================] - 0s 24ms/step\n",
      "VALIDATION METRICS: \n",
      "    accuracy  precision  recall        f1  AUC   duration\n",
      "0       0.5        0.5     1.0  0.666667  0.5  43.044023\n",
      "TEST METRICS: \n",
      "    accuracy  precision  recall        f1  AUC   duration\n",
      "0       0.5        0.5     1.0  0.666667  0.5  43.044023\n",
      "Fold 2 processing complete.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 11520, 1)]           0         []                            \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)             (None, 11520, 64)            576       ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 11520, 64)            256       ['conv1d[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation (Activation)     (None, 11520, 64)            0         ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)           (None, 11520, 64)            20544     ['activation[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 11520, 64)            256       ['conv1d_1[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_1 (Activation)   (None, 11520, 64)            0         ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)           (None, 11520, 64)            128       ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)           (None, 11520, 64)            12352     ['activation_1[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, 11520, 64)            256       ['conv1d_3[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 11520, 64)            256       ['conv1d_2[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 11520, 64)            0         ['batch_normalization_3[0][0]'\n",
      "                                                                    , 'batch_normalization_2[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_2 (Activation)   (None, 11520, 64)            0         ['add[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)           (None, 11520, 128)           65664     ['activation_2[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, 11520, 128)           512       ['conv1d_4[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_3 (Activation)   (None, 11520, 128)           0         ['batch_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)           (None, 11520, 128)           82048     ['activation_3[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, 11520, 128)           512       ['conv1d_5[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_4 (Activation)   (None, 11520, 128)           0         ['batch_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)           (None, 11520, 128)           8320      ['activation_2[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)           (None, 11520, 128)           49280     ['activation_4[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_7 (Bat  (None, 11520, 128)           512       ['conv1d_7[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_6 (Bat  (None, 11520, 128)           512       ['conv1d_6[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, 11520, 128)           0         ['batch_normalization_7[0][0]'\n",
      "                                                                    , 'batch_normalization_6[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_5 (Activation)   (None, 11520, 128)           0         ['add_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_8 (Conv1D)           (None, 11520, 128)           131200    ['activation_5[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_8 (Bat  (None, 11520, 128)           512       ['conv1d_8[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_6 (Activation)   (None, 11520, 128)           0         ['batch_normalization_8[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_9 (Conv1D)           (None, 11520, 128)           82048     ['activation_6[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_9 (Bat  (None, 11520, 128)           512       ['conv1d_9[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_7 (Activation)   (None, 11520, 128)           0         ['batch_normalization_9[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_10 (Conv1D)          (None, 11520, 128)           49280     ['activation_7[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_11 (Ba  (None, 11520, 128)           512       ['activation_5[0][0]']        \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_10 (Ba  (None, 11520, 128)           512       ['conv1d_10[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_2 (Add)                 (None, 11520, 128)           0         ['batch_normalization_11[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'batch_normalization_10[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_8 (Activation)   (None, 11520, 128)           0         ['add_2[0][0]']               \n",
      "                                                                                                  \n",
      " global_average_pooling1d (  (None, 128)                  0         ['activation_8[0][0]']        \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 2)                    258       ['global_average_pooling1d[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 506818 (1.93 MB)\n",
      "Trainable params: 504258 (1.92 MB)\n",
      "Non-trainable params: 2560 (10.00 KB)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7321 - accuracy: 0.6611\n",
      "Epoch 1: val_loss improved from inf to 0.69350, saving model to TFM_MartaRey/ResNet/results_parkinson_real/results/resnet/dataset_real/best_model.hdf5\n",
      "/home/v890/v890699/miniconda3/envs/TFM/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n",
      "10/10 [==============================] - 7s 128ms/step - loss: 0.7321 - accuracy: 0.6611 - val_loss: 0.6935 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5629 - accuracy: 0.7056\n",
      "Epoch 2: val_loss did not improve from 0.69350\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5629 - accuracy: 0.7056 - val_loss: 0.6960 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5828 - accuracy: 0.6944\n",
      "Epoch 3: val_loss did not improve from 0.69350\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5828 - accuracy: 0.6944 - val_loss: 0.6996 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5082 - accuracy: 0.7111\n",
      "Epoch 4: val_loss did not improve from 0.69350\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5082 - accuracy: 0.7111 - val_loss: 0.6995 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4659 - accuracy: 0.7500\n",
      "Epoch 5: val_loss did not improve from 0.69350\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4659 - accuracy: 0.7500 - val_loss: 0.6999 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4493 - accuracy: 0.8056\n",
      "Epoch 6: val_loss did not improve from 0.69350\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.4493 - accuracy: 0.8056 - val_loss: 0.7479 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4456 - accuracy: 0.7611\n",
      "Epoch 7: val_loss did not improve from 0.69350\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4456 - accuracy: 0.7611 - val_loss: 0.7645 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4221 - accuracy: 0.7833\n",
      "Epoch 8: val_loss did not improve from 0.69350\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4221 - accuracy: 0.7833 - val_loss: 0.7993 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4083 - accuracy: 0.7944\n",
      "Epoch 9: val_loss did not improve from 0.69350\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4083 - accuracy: 0.7944 - val_loss: 0.8194 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4028 - accuracy: 0.8222\n",
      "Epoch 10: val_loss did not improve from 0.69350\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4028 - accuracy: 0.8222 - val_loss: 0.8840 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3531 - accuracy: 0.8222\n",
      "Epoch 11: val_loss did not improve from 0.69350\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.3531 - accuracy: 0.8222 - val_loss: 1.0627 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3875 - accuracy: 0.8278\n",
      "Epoch 12: val_loss did not improve from 0.69350\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.3875 - accuracy: 0.8278 - val_loss: 0.6990 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5062 - accuracy: 0.7667\n",
      "Epoch 13: val_loss did not improve from 0.69350\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5062 - accuracy: 0.7667 - val_loss: 0.9895 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3896 - accuracy: 0.8167\n",
      "Epoch 14: val_loss did not improve from 0.69350\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.3896 - accuracy: 0.8167 - val_loss: 0.9771 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3523 - accuracy: 0.8444\n",
      "Epoch 15: val_loss did not improve from 0.69350\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.3523 - accuracy: 0.8444 - val_loss: 1.3177 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4014 - accuracy: 0.8000\n",
      "Epoch 16: val_loss did not improve from 0.69350\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.4014 - accuracy: 0.8000 - val_loss: 0.8305 - val_accuracy: 0.5500 - lr: 0.0010\n",
      "Epoch 17/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4299 - accuracy: 0.7667\n",
      "Epoch 17: val_loss did not improve from 0.69350\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.4299 - accuracy: 0.7667 - val_loss: 0.7926 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 18/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3518 - accuracy: 0.8278\n",
      "Epoch 18: val_loss improved from 0.69350 to 0.67086, saving model to TFM_MartaRey/ResNet/results_parkinson_real/results/resnet/dataset_real/best_model.hdf5\n",
      "10/10 [==============================] - 1s 86ms/step - loss: 0.3518 - accuracy: 0.8278 - val_loss: 0.6709 - val_accuracy: 0.5667 - lr: 0.0010\n",
      "Epoch 19/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4056 - accuracy: 0.7556\n",
      "Epoch 19: val_loss did not improve from 0.67086\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4056 - accuracy: 0.7556 - val_loss: 1.0893 - val_accuracy: 0.5500 - lr: 0.0010\n",
      "Epoch 20/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3547 - accuracy: 0.8167\n",
      "Epoch 20: val_loss did not improve from 0.67086\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.3547 - accuracy: 0.8167 - val_loss: 0.6952 - val_accuracy: 0.5500 - lr: 0.0010\n",
      "Epoch 21/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3184 - accuracy: 0.8611\n",
      "Epoch 21: val_loss did not improve from 0.67086\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.3184 - accuracy: 0.8611 - val_loss: 0.7253 - val_accuracy: 0.5333 - lr: 0.0010\n",
      "Epoch 22/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3687 - accuracy: 0.8167\n",
      "Epoch 22: val_loss did not improve from 0.67086\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.3687 - accuracy: 0.8167 - val_loss: 1.0242 - val_accuracy: 0.5500 - lr: 0.0010\n",
      "Epoch 23/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3343 - accuracy: 0.8611\n",
      "Epoch 23: val_loss did not improve from 0.67086\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.3343 - accuracy: 0.8611 - val_loss: 0.7191 - val_accuracy: 0.5500 - lr: 0.0010\n",
      "Epoch 24/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3379 - accuracy: 0.8389\n",
      "Epoch 24: val_loss did not improve from 0.67086\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.3379 - accuracy: 0.8389 - val_loss: 2.2116 - val_accuracy: 0.5500 - lr: 0.0010\n",
      "Epoch 25/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3718 - accuracy: 0.8333\n",
      "Epoch 25: val_loss did not improve from 0.67086\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.3718 - accuracy: 0.8333 - val_loss: 2.2469 - val_accuracy: 0.5500 - lr: 0.0010\n",
      "Epoch 26/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3237 - accuracy: 0.8389\n",
      "Epoch 26: val_loss did not improve from 0.67086\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.3237 - accuracy: 0.8389 - val_loss: 2.5034 - val_accuracy: 0.5500 - lr: 0.0010\n",
      "Epoch 27/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3478 - accuracy: 0.8389\n",
      "Epoch 27: val_loss did not improve from 0.67086\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.3478 - accuracy: 0.8389 - val_loss: 3.2156 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 28/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3325 - accuracy: 0.8333\n",
      "Epoch 28: val_loss did not improve from 0.67086\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.3325 - accuracy: 0.8333 - val_loss: 2.5352 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 29/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3009 - accuracy: 0.8667\n",
      "Epoch 29: val_loss did not improve from 0.67086\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.3009 - accuracy: 0.8667 - val_loss: 4.0421 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 30/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3100 - accuracy: 0.8667\n",
      "Epoch 30: val_loss did not improve from 0.67086\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.3100 - accuracy: 0.8667 - val_loss: 5.6312 - val_accuracy: 0.5500 - lr: 0.0010\n",
      "Epoch 31/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3302 - accuracy: 0.8444\n",
      "Epoch 31: val_loss did not improve from 0.67086\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.3302 - accuracy: 0.8444 - val_loss: 6.3643 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 32/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3015 - accuracy: 0.8611\n",
      "Epoch 32: val_loss did not improve from 0.67086\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.3015 - accuracy: 0.8611 - val_loss: 5.3045 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 33/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3222 - accuracy: 0.8389\n",
      "Epoch 33: val_loss did not improve from 0.67086\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.3222 - accuracy: 0.8389 - val_loss: 5.3648 - val_accuracy: 0.5500 - lr: 0.0010\n",
      "Epoch 34/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2681 - accuracy: 0.8889\n",
      "Epoch 34: val_loss did not improve from 0.67086\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.2681 - accuracy: 0.8889 - val_loss: 7.7051 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 35/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2084 - accuracy: 0.9222\n",
      "Epoch 35: val_loss did not improve from 0.67086\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.2084 - accuracy: 0.9222 - val_loss: 7.2877 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 36/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2663 - accuracy: 0.9056\n",
      "Epoch 36: val_loss did not improve from 0.67086\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.2663 - accuracy: 0.9056 - val_loss: 5.7028 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 37/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2719 - accuracy: 0.8778\n",
      "Epoch 37: val_loss did not improve from 0.67086\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.2719 - accuracy: 0.8778 - val_loss: 5.1846 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 38/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3736 - accuracy: 0.8667\n",
      "Epoch 38: val_loss did not improve from 0.67086\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.3736 - accuracy: 0.8667 - val_loss: 3.9124 - val_accuracy: 0.5500 - lr: 0.0010\n",
      "Epoch 39/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3598 - accuracy: 0.8556\n",
      "Epoch 39: val_loss did not improve from 0.67086\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.3598 - accuracy: 0.8556 - val_loss: 3.3478 - val_accuracy: 0.5500 - lr: 0.0010\n",
      "Epoch 40/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3010 - accuracy: 0.8667\n",
      "Epoch 40: val_loss did not improve from 0.67086\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.3010 - accuracy: 0.8667 - val_loss: 2.3582 - val_accuracy: 0.5333 - lr: 0.0010\n",
      "Epoch 41/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2845 - accuracy: 0.8667\n",
      "Epoch 41: val_loss did not improve from 0.67086\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.2845 - accuracy: 0.8667 - val_loss: 5.5495 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 42/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2388 - accuracy: 0.8889\n",
      "Epoch 42: val_loss did not improve from 0.67086\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.2388 - accuracy: 0.8889 - val_loss: 3.3057 - val_accuracy: 0.5167 - lr: 0.0010\n",
      "Epoch 43/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2165 - accuracy: 0.9111\n",
      "Epoch 43: val_loss did not improve from 0.67086\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.2165 - accuracy: 0.9111 - val_loss: 4.9520 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 44/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1756 - accuracy: 0.9222\n",
      "Epoch 44: val_loss did not improve from 0.67086\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.1756 - accuracy: 0.9222 - val_loss: 5.7996 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 45/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1906 - accuracy: 0.9222\n",
      "Epoch 45: val_loss did not improve from 0.67086\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.1906 - accuracy: 0.9222 - val_loss: 6.0231 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 46/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2130 - accuracy: 0.8944\n",
      "Epoch 46: val_loss did not improve from 0.67086\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.2130 - accuracy: 0.8944 - val_loss: 3.3363 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 47/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2258 - accuracy: 0.9167\n",
      "Epoch 47: val_loss did not improve from 0.67086\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.2258 - accuracy: 0.9167 - val_loss: 2.2899 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 48/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2130 - accuracy: 0.9278\n",
      "Epoch 48: val_loss did not improve from 0.67086\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.2130 - accuracy: 0.9278 - val_loss: 1.3378 - val_accuracy: 0.5167 - lr: 0.0010\n",
      "Epoch 49/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1469 - accuracy: 0.9333\n",
      "Epoch 49: val_loss did not improve from 0.67086\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.1469 - accuracy: 0.9333 - val_loss: 1.3062 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 50/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2477 - accuracy: 0.9000\n",
      "Epoch 50: val_loss did not improve from 0.67086\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.2477 - accuracy: 0.9000 - val_loss: 2.2902 - val_accuracy: 0.5167 - lr: 0.0010\n",
      "Epoch 51/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2021 - accuracy: 0.9056\n",
      "Epoch 51: val_loss did not improve from 0.67086\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.2021 - accuracy: 0.9056 - val_loss: 8.0637 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 52/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1956 - accuracy: 0.9000\n",
      "Epoch 52: val_loss did not improve from 0.67086\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.1956 - accuracy: 0.9000 - val_loss: 1.8639 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 53/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1129 - accuracy: 0.9722\n",
      "Epoch 53: val_loss did not improve from 0.67086\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.1129 - accuracy: 0.9722 - val_loss: 2.2937 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 54/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1470 - accuracy: 0.9556\n",
      "Epoch 54: val_loss did not improve from 0.67086\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.1470 - accuracy: 0.9556 - val_loss: 2.7951 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 55/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1670 - accuracy: 0.9278\n",
      "Epoch 55: val_loss did not improve from 0.67086\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.1670 - accuracy: 0.9278 - val_loss: 4.0953 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 56/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1472 - accuracy: 0.9500\n",
      "Epoch 56: val_loss did not improve from 0.67086\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.1472 - accuracy: 0.9500 - val_loss: 4.6835 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 57/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1644 - accuracy: 0.9389\n",
      "Epoch 57: val_loss did not improve from 0.67086\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.1644 - accuracy: 0.9389 - val_loss: 3.4157 - val_accuracy: 0.5333 - lr: 0.0010\n",
      "Epoch 58/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1799 - accuracy: 0.9333\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 58: val_loss did not improve from 0.67086\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.1799 - accuracy: 0.9333 - val_loss: 5.0356 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 59/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1410 - accuracy: 0.9278\n",
      "Epoch 59: val_loss did not improve from 0.67086\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.1410 - accuracy: 0.9278 - val_loss: 3.4454 - val_accuracy: 0.5000 - lr: 5.0000e-04\n",
      "Epoch 60/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1151 - accuracy: 0.9611\n",
      "Epoch 60: val_loss did not improve from 0.67086\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.1151 - accuracy: 0.9611 - val_loss: 2.5941 - val_accuracy: 0.5000 - lr: 5.0000e-04\n",
      "Epoch 61/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1361 - accuracy: 0.9444\n",
      "Epoch 61: val_loss did not improve from 0.67086\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.1361 - accuracy: 0.9444 - val_loss: 2.1768 - val_accuracy: 0.5000 - lr: 5.0000e-04\n",
      "Epoch 62/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0836 - accuracy: 0.9778\n",
      "Epoch 62: val_loss did not improve from 0.67086\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.0836 - accuracy: 0.9778 - val_loss: 2.2700 - val_accuracy: 0.5000 - lr: 5.0000e-04\n",
      "Epoch 63/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0867 - accuracy: 0.9667\n",
      "Epoch 63: val_loss did not improve from 0.67086\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.0867 - accuracy: 0.9667 - val_loss: 2.7957 - val_accuracy: 0.5000 - lr: 5.0000e-04\n",
      "Epoch 64/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0591 - accuracy: 0.9944\n",
      "Epoch 64: val_loss did not improve from 0.67086\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.0591 - accuracy: 0.9944 - val_loss: 2.7939 - val_accuracy: 0.5000 - lr: 5.0000e-04\n",
      "Epoch 65/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0663 - accuracy: 0.9833\n",
      "Epoch 65: val_loss did not improve from 0.67086\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.0663 - accuracy: 0.9833 - val_loss: 3.5506 - val_accuracy: 0.5000 - lr: 5.0000e-04\n",
      "Epoch 66/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.1017 - accuracy: 0.9611\n",
      "Epoch 66: val_loss did not improve from 0.67086\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.1017 - accuracy: 0.9611 - val_loss: 2.1761 - val_accuracy: 0.5333 - lr: 5.0000e-04\n",
      "Epoch 67/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0853 - accuracy: 0.9722\n",
      "Epoch 67: val_loss did not improve from 0.67086\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.0853 - accuracy: 0.9722 - val_loss: 3.4903 - val_accuracy: 0.5000 - lr: 5.0000e-04\n",
      "Epoch 68/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.0524 - accuracy: 0.9944\n",
      "Epoch 68: val_loss did not improve from 0.67086\n",
      "Restoring model weights from the end of the best epoch: 18.\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 0.0524 - accuracy: 0.9944 - val_loss: 3.9214 - val_accuracy: 0.5000 - lr: 5.0000e-04\n",
      "Epoch 68: early stopping\n",
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x154d1a626de0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 24ms/step\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x1542f4e1cfe0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2/2 [==============================] - 0s 30ms/step\n",
      "VALIDATION METRICS: \n",
      "    accuracy  precision  recall        f1       AUC   duration\n",
      "0  0.566667   0.535714     1.0  0.697674  0.566667  55.333621\n",
      "TEST METRICS: \n",
      "    accuracy  precision    recall        f1       AUC   duration\n",
      "0  0.616667   0.568627  0.966667  0.716049  0.616667  55.333621\n",
      "Fold 3 processing complete.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 11520, 1)]           0         []                            \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)             (None, 11520, 64)            576       ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 11520, 64)            256       ['conv1d[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation (Activation)     (None, 11520, 64)            0         ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)           (None, 11520, 64)            20544     ['activation[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 11520, 64)            256       ['conv1d_1[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_1 (Activation)   (None, 11520, 64)            0         ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)           (None, 11520, 64)            128       ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)           (None, 11520, 64)            12352     ['activation_1[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, 11520, 64)            256       ['conv1d_3[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 11520, 64)            256       ['conv1d_2[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 11520, 64)            0         ['batch_normalization_3[0][0]'\n",
      "                                                                    , 'batch_normalization_2[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_2 (Activation)   (None, 11520, 64)            0         ['add[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)           (None, 11520, 128)           65664     ['activation_2[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, 11520, 128)           512       ['conv1d_4[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_3 (Activation)   (None, 11520, 128)           0         ['batch_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)           (None, 11520, 128)           82048     ['activation_3[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, 11520, 128)           512       ['conv1d_5[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_4 (Activation)   (None, 11520, 128)           0         ['batch_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)           (None, 11520, 128)           8320      ['activation_2[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)           (None, 11520, 128)           49280     ['activation_4[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_7 (Bat  (None, 11520, 128)           512       ['conv1d_7[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_6 (Bat  (None, 11520, 128)           512       ['conv1d_6[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, 11520, 128)           0         ['batch_normalization_7[0][0]'\n",
      "                                                                    , 'batch_normalization_6[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_5 (Activation)   (None, 11520, 128)           0         ['add_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_8 (Conv1D)           (None, 11520, 128)           131200    ['activation_5[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_8 (Bat  (None, 11520, 128)           512       ['conv1d_8[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_6 (Activation)   (None, 11520, 128)           0         ['batch_normalization_8[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_9 (Conv1D)           (None, 11520, 128)           82048     ['activation_6[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_9 (Bat  (None, 11520, 128)           512       ['conv1d_9[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_7 (Activation)   (None, 11520, 128)           0         ['batch_normalization_9[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_10 (Conv1D)          (None, 11520, 128)           49280     ['activation_7[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_11 (Ba  (None, 11520, 128)           512       ['activation_5[0][0]']        \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_10 (Ba  (None, 11520, 128)           512       ['conv1d_10[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_2 (Add)                 (None, 11520, 128)           0         ['batch_normalization_11[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'batch_normalization_10[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_8 (Activation)   (None, 11520, 128)           0         ['add_2[0][0]']               \n",
      "                                                                                                  \n",
      " global_average_pooling1d (  (None, 128)                  0         ['activation_8[0][0]']        \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 2)                    258       ['global_average_pooling1d[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 506818 (1.93 MB)\n",
      "Trainable params: 504258 (1.92 MB)\n",
      "Non-trainable params: 2560 (10.00 KB)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.7297 - accuracy: 0.6333\n",
      "Epoch 1: val_loss improved from inf to 0.69215, saving model to TFM_MartaRey/ResNet/results_parkinson_real/results/resnet/dataset_real/best_model.hdf5\n",
      "/home/v890/v890699/miniconda3/envs/TFM/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n",
      "10/10 [==============================] - 7s 129ms/step - loss: 0.7297 - accuracy: 0.6333 - val_loss: 0.6922 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5612 - accuracy: 0.7333\n",
      "Epoch 2: val_loss did not improve from 0.69215\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5612 - accuracy: 0.7333 - val_loss: 0.6922 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5391 - accuracy: 0.7333\n",
      "Epoch 3: val_loss did not improve from 0.69215\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5391 - accuracy: 0.7333 - val_loss: 0.7112 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5603 - accuracy: 0.7556\n",
      "Epoch 4: val_loss did not improve from 0.69215\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5603 - accuracy: 0.7556 - val_loss: 0.7009 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5343 - accuracy: 0.6833\n",
      "Epoch 5: val_loss did not improve from 0.69215\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5343 - accuracy: 0.6833 - val_loss: 0.7965 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5352 - accuracy: 0.7667\n",
      "Epoch 6: val_loss did not improve from 0.69215\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5352 - accuracy: 0.7667 - val_loss: 0.8708 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4950 - accuracy: 0.7389\n",
      "Epoch 7: val_loss did not improve from 0.69215\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4950 - accuracy: 0.7389 - val_loss: 0.8941 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5223 - accuracy: 0.7611\n",
      "Epoch 8: val_loss did not improve from 0.69215\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.5223 - accuracy: 0.7611 - val_loss: 0.9845 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5236 - accuracy: 0.7111\n",
      "Epoch 9: val_loss did not improve from 0.69215\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5236 - accuracy: 0.7111 - val_loss: 1.0881 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5121 - accuracy: 0.7222\n",
      "Epoch 10: val_loss did not improve from 0.69215\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.5121 - accuracy: 0.7222 - val_loss: 1.3281 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5243 - accuracy: 0.7278\n",
      "Epoch 11: val_loss did not improve from 0.69215\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.5243 - accuracy: 0.7278 - val_loss: 1.5060 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5135 - accuracy: 0.7333\n",
      "Epoch 12: val_loss did not improve from 0.69215\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5135 - accuracy: 0.7333 - val_loss: 1.4417 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5028 - accuracy: 0.7333\n",
      "Epoch 13: val_loss did not improve from 0.69215\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5028 - accuracy: 0.7333 - val_loss: 1.7224 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4996 - accuracy: 0.7833\n",
      "Epoch 14: val_loss did not improve from 0.69215\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4996 - accuracy: 0.7833 - val_loss: 1.5967 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5114 - accuracy: 0.7556\n",
      "Epoch 15: val_loss did not improve from 0.69215\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5114 - accuracy: 0.7556 - val_loss: 1.5700 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4594 - accuracy: 0.7722\n",
      "Epoch 16: val_loss did not improve from 0.69215\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.4594 - accuracy: 0.7722 - val_loss: 1.8259 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 17/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4791 - accuracy: 0.7444\n",
      "Epoch 17: val_loss did not improve from 0.69215\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.4791 - accuracy: 0.7444 - val_loss: 1.9843 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 18/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5415 - accuracy: 0.6944\n",
      "Epoch 18: val_loss did not improve from 0.69215\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5415 - accuracy: 0.6944 - val_loss: 1.5671 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 19/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4928 - accuracy: 0.7667\n",
      "Epoch 19: val_loss did not improve from 0.69215\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4928 - accuracy: 0.7667 - val_loss: 1.2134 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 20/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4842 - accuracy: 0.7444\n",
      "Epoch 20: val_loss did not improve from 0.69215\n",
      "10/10 [==============================] - 1s 74ms/step - loss: 0.4842 - accuracy: 0.7444 - val_loss: 1.2793 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 21/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4504 - accuracy: 0.7889\n",
      "Epoch 21: val_loss did not improve from 0.69215\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.4504 - accuracy: 0.7889 - val_loss: 1.9275 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 22/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4698 - accuracy: 0.7611\n",
      "Epoch 22: val_loss did not improve from 0.69215\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.4698 - accuracy: 0.7611 - val_loss: 2.2724 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 23/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4478 - accuracy: 0.8000\n",
      "Epoch 23: val_loss did not improve from 0.69215\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.4478 - accuracy: 0.8000 - val_loss: 1.8428 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 24/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4199 - accuracy: 0.7944\n",
      "Epoch 24: val_loss did not improve from 0.69215\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.4199 - accuracy: 0.7944 - val_loss: 2.2587 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 25/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4456 - accuracy: 0.7611\n",
      "Epoch 25: val_loss did not improve from 0.69215\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4456 - accuracy: 0.7611 - val_loss: 2.4165 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 26/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3950 - accuracy: 0.8278\n",
      "Epoch 26: val_loss did not improve from 0.69215\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.3950 - accuracy: 0.8278 - val_loss: 2.9589 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 27/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4502 - accuracy: 0.7722\n",
      "Epoch 27: val_loss did not improve from 0.69215\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4502 - accuracy: 0.7722 - val_loss: 3.1866 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 28/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3997 - accuracy: 0.8222\n",
      "Epoch 28: val_loss did not improve from 0.69215\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.3997 - accuracy: 0.8222 - val_loss: 3.1730 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 29/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4745 - accuracy: 0.7500\n",
      "Epoch 29: val_loss did not improve from 0.69215\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.4745 - accuracy: 0.7500 - val_loss: 2.8695 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 30/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4339 - accuracy: 0.7944\n",
      "Epoch 30: val_loss did not improve from 0.69215\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.4339 - accuracy: 0.7944 - val_loss: 3.7342 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 31/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4621 - accuracy: 0.7722\n",
      "Epoch 31: val_loss did not improve from 0.69215\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.4621 - accuracy: 0.7722 - val_loss: 3.8139 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 32/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4205 - accuracy: 0.7833\n",
      "Epoch 32: val_loss did not improve from 0.69215\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.4205 - accuracy: 0.7833 - val_loss: 4.2301 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 33/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3818 - accuracy: 0.8111\n",
      "Epoch 33: val_loss did not improve from 0.69215\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.3818 - accuracy: 0.8111 - val_loss: 5.2826 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 34/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4387 - accuracy: 0.7667\n",
      "Epoch 34: val_loss did not improve from 0.69215\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.4387 - accuracy: 0.7667 - val_loss: 4.3072 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 35/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3918 - accuracy: 0.8000\n",
      "Epoch 35: val_loss did not improve from 0.69215\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.3918 - accuracy: 0.8000 - val_loss: 4.1238 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 36/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3912 - accuracy: 0.8056\n",
      "Epoch 36: val_loss did not improve from 0.69215\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.3912 - accuracy: 0.8056 - val_loss: 4.7964 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 37/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4111 - accuracy: 0.7944\n",
      "Epoch 37: val_loss did not improve from 0.69215\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.4111 - accuracy: 0.7944 - val_loss: 5.6945 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 38/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4470 - accuracy: 0.7889\n",
      "Epoch 38: val_loss did not improve from 0.69215\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.4470 - accuracy: 0.7889 - val_loss: 5.4127 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 39/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4025 - accuracy: 0.8278\n",
      "Epoch 39: val_loss did not improve from 0.69215\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.4025 - accuracy: 0.8278 - val_loss: 4.2443 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 40/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4097 - accuracy: 0.8000\n",
      "Epoch 40: val_loss did not improve from 0.69215\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.4097 - accuracy: 0.8000 - val_loss: 4.5660 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 41/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3853 - accuracy: 0.8222\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.69215\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.3853 - accuracy: 0.8222 - val_loss: 5.1363 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 42/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3974 - accuracy: 0.8111\n",
      "Epoch 42: val_loss did not improve from 0.69215\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.3974 - accuracy: 0.8111 - val_loss: 5.3139 - val_accuracy: 0.5000 - lr: 5.0000e-04\n",
      "Epoch 43/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3842 - accuracy: 0.8056\n",
      "Epoch 43: val_loss did not improve from 0.69215\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.3842 - accuracy: 0.8056 - val_loss: 4.9765 - val_accuracy: 0.5000 - lr: 5.0000e-04\n",
      "Epoch 44/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3604 - accuracy: 0.8222\n",
      "Epoch 44: val_loss did not improve from 0.69215\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.3604 - accuracy: 0.8222 - val_loss: 5.3730 - val_accuracy: 0.5000 - lr: 5.0000e-04\n",
      "Epoch 45/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3102 - accuracy: 0.8889\n",
      "Epoch 45: val_loss did not improve from 0.69215\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.3102 - accuracy: 0.8889 - val_loss: 4.9677 - val_accuracy: 0.5000 - lr: 5.0000e-04\n",
      "Epoch 46/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3594 - accuracy: 0.8611\n",
      "Epoch 46: val_loss did not improve from 0.69215\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.3594 - accuracy: 0.8611 - val_loss: 3.7112 - val_accuracy: 0.5000 - lr: 5.0000e-04\n",
      "Epoch 47/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3221 - accuracy: 0.8556\n",
      "Epoch 47: val_loss did not improve from 0.69215\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.3221 - accuracy: 0.8556 - val_loss: 2.8335 - val_accuracy: 0.5000 - lr: 5.0000e-04\n",
      "Epoch 48/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3009 - accuracy: 0.8722\n",
      "Epoch 48: val_loss did not improve from 0.69215\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.3009 - accuracy: 0.8722 - val_loss: 2.6246 - val_accuracy: 0.5000 - lr: 5.0000e-04\n",
      "Epoch 49/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2866 - accuracy: 0.8611\n",
      "Epoch 49: val_loss did not improve from 0.69215\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.2866 - accuracy: 0.8611 - val_loss: 3.2099 - val_accuracy: 0.5000 - lr: 5.0000e-04\n",
      "Epoch 50/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3568 - accuracy: 0.8167\n",
      "Epoch 50: val_loss did not improve from 0.69215\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.3568 - accuracy: 0.8167 - val_loss: 0.8442 - val_accuracy: 0.6167 - lr: 5.0000e-04\n",
      "Epoch 51/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3113 - accuracy: 0.8444\n",
      "Epoch 51: val_loss did not improve from 0.69215\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "10/10 [==============================] - 1s 77ms/step - loss: 0.3113 - accuracy: 0.8444 - val_loss: 1.1614 - val_accuracy: 0.6167 - lr: 5.0000e-04\n",
      "Epoch 51: early stopping\n",
      "2/2 [==============================] - 0s 21ms/step\n",
      "2/2 [==============================] - 0s 30ms/step\n",
      "/home/v890/v890699/miniconda3/envs/TFM/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/v890/v890699/miniconda3/envs/TFM/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/v890/v890699/miniconda3/envs/TFM/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/v890/v890699/miniconda3/envs/TFM/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "VALIDATION METRICS: \n",
      "    accuracy  precision  recall   f1  AUC   duration\n",
      "0       0.5        0.0     0.0  0.0  0.5  42.915623\n",
      "TEST METRICS: \n",
      "    accuracy  precision  recall   f1  AUC   duration\n",
      "0       0.5        0.0     0.0  0.0  0.5  42.915623\n",
      "Fold 4 processing complete.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 11520, 1)]           0         []                            \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)             (None, 11520, 64)            576       ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 11520, 64)            256       ['conv1d[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation (Activation)     (None, 11520, 64)            0         ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)           (None, 11520, 64)            20544     ['activation[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 11520, 64)            256       ['conv1d_1[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_1 (Activation)   (None, 11520, 64)            0         ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)           (None, 11520, 64)            128       ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)           (None, 11520, 64)            12352     ['activation_1[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, 11520, 64)            256       ['conv1d_3[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 11520, 64)            256       ['conv1d_2[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 11520, 64)            0         ['batch_normalization_3[0][0]'\n",
      "                                                                    , 'batch_normalization_2[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_2 (Activation)   (None, 11520, 64)            0         ['add[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)           (None, 11520, 128)           65664     ['activation_2[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, 11520, 128)           512       ['conv1d_4[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_3 (Activation)   (None, 11520, 128)           0         ['batch_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)           (None, 11520, 128)           82048     ['activation_3[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, 11520, 128)           512       ['conv1d_5[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_4 (Activation)   (None, 11520, 128)           0         ['batch_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)           (None, 11520, 128)           8320      ['activation_2[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)           (None, 11520, 128)           49280     ['activation_4[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_7 (Bat  (None, 11520, 128)           512       ['conv1d_7[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_6 (Bat  (None, 11520, 128)           512       ['conv1d_6[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, 11520, 128)           0         ['batch_normalization_7[0][0]'\n",
      "                                                                    , 'batch_normalization_6[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_5 (Activation)   (None, 11520, 128)           0         ['add_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_8 (Conv1D)           (None, 11520, 128)           131200    ['activation_5[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_8 (Bat  (None, 11520, 128)           512       ['conv1d_8[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_6 (Activation)   (None, 11520, 128)           0         ['batch_normalization_8[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_9 (Conv1D)           (None, 11520, 128)           82048     ['activation_6[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_9 (Bat  (None, 11520, 128)           512       ['conv1d_9[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_7 (Activation)   (None, 11520, 128)           0         ['batch_normalization_9[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_10 (Conv1D)          (None, 11520, 128)           49280     ['activation_7[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_11 (Ba  (None, 11520, 128)           512       ['activation_5[0][0]']        \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_10 (Ba  (None, 11520, 128)           512       ['conv1d_10[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_2 (Add)                 (None, 11520, 128)           0         ['batch_normalization_11[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'batch_normalization_10[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_8 (Activation)   (None, 11520, 128)           0         ['add_2[0][0]']               \n",
      "                                                                                                  \n",
      " global_average_pooling1d (  (None, 128)                  0         ['activation_8[0][0]']        \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 2)                    258       ['global_average_pooling1d[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 506818 (1.93 MB)\n",
      "Trainable params: 504258 (1.92 MB)\n",
      "Non-trainable params: 2560 (10.00 KB)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.8619 - accuracy: 0.5833\n",
      "Epoch 1: val_loss improved from inf to 0.69613, saving model to TFM_MartaRey/ResNet/results_parkinson_real/results/resnet/dataset_real/best_model.hdf5\n",
      "/home/v890/v890699/miniconda3/envs/TFM/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n",
      "10/10 [==============================] - 7s 129ms/step - loss: 0.8619 - accuracy: 0.5833 - val_loss: 0.6961 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.6051 - accuracy: 0.6778\n",
      "Epoch 2: val_loss did not improve from 0.69613\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.6051 - accuracy: 0.6778 - val_loss: 0.6983 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5880 - accuracy: 0.6611\n",
      "Epoch 3: val_loss did not improve from 0.69613\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5880 - accuracy: 0.6611 - val_loss: 0.7076 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5453 - accuracy: 0.7167\n",
      "Epoch 4: val_loss did not improve from 0.69613\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5453 - accuracy: 0.7167 - val_loss: 0.7397 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5496 - accuracy: 0.7167\n",
      "Epoch 5: val_loss did not improve from 0.69613\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5496 - accuracy: 0.7167 - val_loss: 0.7293 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4847 - accuracy: 0.7611\n",
      "Epoch 6: val_loss did not improve from 0.69613\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4847 - accuracy: 0.7611 - val_loss: 0.7619 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5032 - accuracy: 0.7222\n",
      "Epoch 7: val_loss did not improve from 0.69613\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5032 - accuracy: 0.7222 - val_loss: 0.8054 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4819 - accuracy: 0.7556\n",
      "Epoch 8: val_loss did not improve from 0.69613\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.4819 - accuracy: 0.7556 - val_loss: 0.8202 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5050 - accuracy: 0.7611\n",
      "Epoch 9: val_loss did not improve from 0.69613\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.5050 - accuracy: 0.7611 - val_loss: 0.9178 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5079 - accuracy: 0.7278\n",
      "Epoch 10: val_loss did not improve from 0.69613\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5079 - accuracy: 0.7278 - val_loss: 0.9790 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5160 - accuracy: 0.6833\n",
      "Epoch 11: val_loss did not improve from 0.69613\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5160 - accuracy: 0.6833 - val_loss: 0.9747 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5062 - accuracy: 0.7278\n",
      "Epoch 12: val_loss did not improve from 0.69613\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5062 - accuracy: 0.7278 - val_loss: 0.8639 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4718 - accuracy: 0.7778\n",
      "Epoch 13: val_loss did not improve from 0.69613\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.4718 - accuracy: 0.7778 - val_loss: 0.8461 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4837 - accuracy: 0.7333\n",
      "Epoch 14: val_loss did not improve from 0.69613\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.4837 - accuracy: 0.7333 - val_loss: 0.9279 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4750 - accuracy: 0.7833\n",
      "Epoch 15: val_loss did not improve from 0.69613\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4750 - accuracy: 0.7833 - val_loss: 1.0727 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4402 - accuracy: 0.7667\n",
      "Epoch 16: val_loss did not improve from 0.69613\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4402 - accuracy: 0.7667 - val_loss: 1.2078 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 17/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4353 - accuracy: 0.7722\n",
      "Epoch 17: val_loss did not improve from 0.69613\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.4353 - accuracy: 0.7722 - val_loss: 1.0842 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 18/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4436 - accuracy: 0.8167\n",
      "Epoch 18: val_loss did not improve from 0.69613\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4436 - accuracy: 0.8167 - val_loss: 0.7602 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 19/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.5451 - accuracy: 0.7389\n",
      "Epoch 19: val_loss did not improve from 0.69613\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.5451 - accuracy: 0.7389 - val_loss: 0.7862 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 20/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4709 - accuracy: 0.7444\n",
      "Epoch 20: val_loss did not improve from 0.69613\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4709 - accuracy: 0.7444 - val_loss: 0.7458 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 21/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4637 - accuracy: 0.7667\n",
      "Epoch 21: val_loss did not improve from 0.69613\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.4637 - accuracy: 0.7667 - val_loss: 0.7391 - val_accuracy: 0.4833 - lr: 0.0010\n",
      "Epoch 22/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3967 - accuracy: 0.8444\n",
      "Epoch 22: val_loss did not improve from 0.69613\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.3967 - accuracy: 0.8444 - val_loss: 0.7551 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 23/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4256 - accuracy: 0.7833\n",
      "Epoch 23: val_loss did not improve from 0.69613\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4256 - accuracy: 0.7833 - val_loss: 1.1442 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 24/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4590 - accuracy: 0.7778\n",
      "Epoch 24: val_loss did not improve from 0.69613\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4590 - accuracy: 0.7778 - val_loss: 0.9718 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 25/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4359 - accuracy: 0.7833\n",
      "Epoch 25: val_loss did not improve from 0.69613\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4359 - accuracy: 0.7833 - val_loss: 2.2714 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 26/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4362 - accuracy: 0.8000\n",
      "Epoch 26: val_loss did not improve from 0.69613\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4362 - accuracy: 0.8000 - val_loss: 2.3282 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 27/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3727 - accuracy: 0.8333\n",
      "Epoch 27: val_loss did not improve from 0.69613\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.3727 - accuracy: 0.8333 - val_loss: 1.7891 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 28/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4182 - accuracy: 0.8056\n",
      "Epoch 28: val_loss did not improve from 0.69613\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4182 - accuracy: 0.8056 - val_loss: 2.1407 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 29/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4022 - accuracy: 0.8111\n",
      "Epoch 29: val_loss did not improve from 0.69613\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.4022 - accuracy: 0.8111 - val_loss: 4.6205 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 30/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3478 - accuracy: 0.8333\n",
      "Epoch 30: val_loss did not improve from 0.69613\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.3478 - accuracy: 0.8333 - val_loss: 2.8011 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 31/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3584 - accuracy: 0.8500\n",
      "Epoch 31: val_loss did not improve from 0.69613\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.3584 - accuracy: 0.8500 - val_loss: 6.3450 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 32/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3795 - accuracy: 0.8500\n",
      "Epoch 32: val_loss did not improve from 0.69613\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.3795 - accuracy: 0.8500 - val_loss: 4.2454 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 33/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3342 - accuracy: 0.8500\n",
      "Epoch 33: val_loss did not improve from 0.69613\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.3342 - accuracy: 0.8500 - val_loss: 6.7708 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 34/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2853 - accuracy: 0.8833\n",
      "Epoch 34: val_loss did not improve from 0.69613\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.2853 - accuracy: 0.8833 - val_loss: 6.2725 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 35/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3683 - accuracy: 0.8667\n",
      "Epoch 35: val_loss did not improve from 0.69613\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.3683 - accuracy: 0.8667 - val_loss: 8.2657 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 36/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3455 - accuracy: 0.8833\n",
      "Epoch 36: val_loss did not improve from 0.69613\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.3455 - accuracy: 0.8833 - val_loss: 6.9941 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 37/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3476 - accuracy: 0.8333\n",
      "Epoch 37: val_loss did not improve from 0.69613\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.3476 - accuracy: 0.8333 - val_loss: 8.2871 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 38/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3048 - accuracy: 0.8500\n",
      "Epoch 38: val_loss did not improve from 0.69613\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.3048 - accuracy: 0.8500 - val_loss: 8.6028 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 39/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2808 - accuracy: 0.8944\n",
      "Epoch 39: val_loss did not improve from 0.69613\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.2808 - accuracy: 0.8944 - val_loss: 7.8161 - val_accuracy: 0.5167 - lr: 0.0010\n",
      "Epoch 40/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2876 - accuracy: 0.9056\n",
      "Epoch 40: val_loss did not improve from 0.69613\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.2876 - accuracy: 0.9056 - val_loss: 9.8604 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 41/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3581 - accuracy: 0.8778\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 41: val_loss did not improve from 0.69613\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.3581 - accuracy: 0.8778 - val_loss: 9.7111 - val_accuracy: 0.5000 - lr: 0.0010\n",
      "Epoch 42/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.4131 - accuracy: 0.8333\n",
      "Epoch 42: val_loss did not improve from 0.69613\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.4131 - accuracy: 0.8333 - val_loss: 9.6953 - val_accuracy: 0.5000 - lr: 5.0000e-04\n",
      "Epoch 43/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2984 - accuracy: 0.8722\n",
      "Epoch 43: val_loss did not improve from 0.69613\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.2984 - accuracy: 0.8722 - val_loss: 9.3290 - val_accuracy: 0.5167 - lr: 5.0000e-04\n",
      "Epoch 44/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3029 - accuracy: 0.8667\n",
      "Epoch 44: val_loss did not improve from 0.69613\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 0.3029 - accuracy: 0.8667 - val_loss: 7.0531 - val_accuracy: 0.4833 - lr: 5.0000e-04\n",
      "Epoch 45/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3129 - accuracy: 0.8333\n",
      "Epoch 45: val_loss did not improve from 0.69613\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.3129 - accuracy: 0.8333 - val_loss: 6.9813 - val_accuracy: 0.5000 - lr: 5.0000e-04\n",
      "Epoch 46/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2621 - accuracy: 0.8889\n",
      "Epoch 46: val_loss did not improve from 0.69613\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.2621 - accuracy: 0.8889 - val_loss: 9.2801 - val_accuracy: 0.5167 - lr: 5.0000e-04\n",
      "Epoch 47/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2536 - accuracy: 0.8833\n",
      "Epoch 47: val_loss did not improve from 0.69613\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.2536 - accuracy: 0.8833 - val_loss: 11.6181 - val_accuracy: 0.5167 - lr: 5.0000e-04\n",
      "Epoch 48/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2618 - accuracy: 0.8889\n",
      "Epoch 48: val_loss did not improve from 0.69613\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.2618 - accuracy: 0.8889 - val_loss: 10.9105 - val_accuracy: 0.5167 - lr: 5.0000e-04\n",
      "Epoch 49/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2239 - accuracy: 0.8889\n",
      "Epoch 49: val_loss did not improve from 0.69613\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.2239 - accuracy: 0.8889 - val_loss: 12.0268 - val_accuracy: 0.5333 - lr: 5.0000e-04\n",
      "Epoch 50/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2095 - accuracy: 0.9056\n",
      "Epoch 50: val_loss did not improve from 0.69613\n",
      "10/10 [==============================] - 1s 72ms/step - loss: 0.2095 - accuracy: 0.9056 - val_loss: 10.1774 - val_accuracy: 0.5167 - lr: 5.0000e-04\n",
      "Epoch 51/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.2675 - accuracy: 0.8611\n",
      "Epoch 51: val_loss did not improve from 0.69613\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 0.2675 - accuracy: 0.8611 - val_loss: 9.3633 - val_accuracy: 0.5000 - lr: 5.0000e-04\n",
      "Epoch 51: early stopping\n",
      "2/2 [==============================] - 0s 21ms/step\n",
      "2/2 [==============================] - 0s 22ms/step\n",
      "VALIDATION METRICS: \n",
      "    accuracy  precision  recall        f1  AUC   duration\n",
      "0       0.5        0.5     1.0  0.666667  0.5  43.126429\n",
      "TEST METRICS: \n",
      "    accuracy  precision  recall        f1  AUC   duration\n",
      "0       0.5        0.5     1.0  0.666667  0.5  43.126429\n",
      "Fold 5 processing complete.\n",
      "/home/v890/v890699/miniconda3/envs/TFM/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/media/beegfs/home/v890/v890699/TFM_MartaRey/ResNet/classifiers/resnet_folds.py:262: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  label='Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_metrics[4], std_metrics[4]),\n",
      "All folds processed and saved.\n",
      "Classification process completed for all folds.\n"
     ]
    }
   ],
   "source": [
    "!python TFM_MartaRey/ResNet/main_folds.py resnet real TFM_MartaRey/ResNet/results_parkinson_real\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carpeta 'TFM_MartaRey/ResNet/results_parkinson_real_1106' eliminada.\n",
      "Carpeta 'TFM_MartaRey/ResNet/results_parkinson_real_1106' creada.\n",
      "Method:  resnet\n",
      "2024-06-11 19:53:27.101865: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-11 19:53:27.280210: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-11 19:53:27.280382: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-11 19:53:27.303934: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-11 19:53:27.383827: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-11 19:53:30.226293: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-06-11 19:53:34.539544: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38374 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:86:00.0, compute capability: 8.0\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 11520, 1)]           0         []                            \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)             (None, 11520, 64)            576       ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 11520, 64)            256       ['conv1d[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation (Activation)     (None, 11520, 64)            0         ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)           (None, 11520, 64)            20544     ['activation[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 11520, 64)            256       ['conv1d_1[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_1 (Activation)   (None, 11520, 64)            0         ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)           (None, 11520, 64)            128       ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)           (None, 11520, 64)            12352     ['activation_1[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, 11520, 64)            256       ['conv1d_3[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 11520, 64)            256       ['conv1d_2[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 11520, 64)            0         ['batch_normalization_3[0][0]'\n",
      "                                                                    , 'batch_normalization_2[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_2 (Activation)   (None, 11520, 64)            0         ['add[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)           (None, 11520, 128)           65664     ['activation_2[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, 11520, 128)           512       ['conv1d_4[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_3 (Activation)   (None, 11520, 128)           0         ['batch_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)           (None, 11520, 128)           82048     ['activation_3[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, 11520, 128)           512       ['conv1d_5[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_4 (Activation)   (None, 11520, 128)           0         ['batch_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)           (None, 11520, 128)           8320      ['activation_2[0][0]']        \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)           (None, 11520, 128)           49280     ['activation_4[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_7 (Bat  (None, 11520, 128)           512       ['conv1d_7[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_6 (Bat  (None, 11520, 128)           512       ['conv1d_6[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, 11520, 128)           0         ['batch_normalization_7[0][0]'\n",
      "                                                                    , 'batch_normalization_6[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_5 (Activation)   (None, 11520, 128)           0         ['add_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_8 (Conv1D)           (None, 11520, 128)           131200    ['activation_5[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_8 (Bat  (None, 11520, 128)           512       ['conv1d_8[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_6 (Activation)   (None, 11520, 128)           0         ['batch_normalization_8[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_9 (Conv1D)           (None, 11520, 128)           82048     ['activation_6[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_9 (Bat  (None, 11520, 128)           512       ['conv1d_9[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_7 (Activation)   (None, 11520, 128)           0         ['batch_normalization_9[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_10 (Conv1D)          (None, 11520, 128)           49280     ['activation_7[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_11 (Ba  (None, 11520, 128)           512       ['activation_5[0][0]']        \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_10 (Ba  (None, 11520, 128)           512       ['conv1d_10[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_2 (Add)                 (None, 11520, 128)           0         ['batch_normalization_11[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'batch_normalization_10[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_8 (Activation)   (None, 11520, 128)           0         ['add_2[0][0]']               \n",
      "                                                                                                  \n",
      " global_average_pooling1d (  (None, 128)                  0         ['activation_8[0][0]']        \n",
      " GlobalAveragePooling1D)                                                                          \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 2)                    258       ['global_average_pooling1d[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 506818 (1.93 MB)\n",
      "Trainable params: 504258 (1.92 MB)\n",
      "Non-trainable params: 2560 (10.00 KB)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/500\n",
      "2024-06-11 19:53:41.518919: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:447] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.9.4.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2024-06-11 19:53:41.519961: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at conv_ops_impl.h:1199 : UNIMPLEMENTED: DNN library is not found.\n",
      "2024-06-11 19:53:41.524474: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:447] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.9.4.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2024-06-11 19:53:41.525048: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at conv_ops_impl.h:1199 : UNIMPLEMENTED: DNN library is not found.\n",
      "Traceback (most recent call last):\n",
      "  File \"/media/beegfs/home/v890/v890699/TFM_MartaRey/ResNet/main_folds.py\", line 107, in <module>\n",
      "    fit_classifier(classifier_name, output_directory, dataset_id, id)\n",
      "  File \"/media/beegfs/home/v890/v890699/TFM_MartaRey/ResNet/main_folds.py\", line 61, in fit_classifier\n",
      "    classifier.fit(x_train, y_train, x_val, y_val, y_true_val, x_test, y_test, y_true_test, fold_index)\n",
      "  File \"/media/beegfs/home/v890/v890699/TFM_MartaRey/ResNet/classifiers/resnet_folds.py\", line 150, in fit\n",
      "    hist = self.model.fit(x_train, y_train, batch_size=mini_batch_size, epochs=nb_epochs,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/v890/v890699/miniconda3/envs/TFM_new/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"/home/v890/v890699/miniconda3/envs/TFM_new/lib/python3.11/site-packages/tensorflow/python/eager/execute.py\", line 53, in quick_execute\n",
      "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
      "  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "tensorflow.python.framework.errors_impl.UnimplementedError: Graph execution error:\n",
      "\n",
      "Detected at node model/conv1d/Conv1D defined at (most recent call last):\n",
      "  File \"/media/beegfs/home/v890/v890699/TFM_MartaRey/ResNet/main_folds.py\", line 107, in <module>\n",
      "\n",
      "  File \"/media/beegfs/home/v890/v890699/TFM_MartaRey/ResNet/main_folds.py\", line 61, in fit_classifier\n",
      "\n",
      "  File \"/media/beegfs/home/v890/v890699/TFM_MartaRey/ResNet/classifiers/resnet_folds.py\", line 150, in fit\n",
      "\n",
      "  File \"/home/v890/v890699/miniconda3/envs/TFM_new/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n",
      "\n",
      "  File \"/home/v890/v890699/miniconda3/envs/TFM_new/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1807, in fit\n",
      "\n",
      "  File \"/home/v890/v890699/miniconda3/envs/TFM_new/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1401, in train_function\n",
      "\n",
      "  File \"/home/v890/v890699/miniconda3/envs/TFM_new/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1384, in step_function\n",
      "\n",
      "  File \"/home/v890/v890699/miniconda3/envs/TFM_new/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1373, in run_step\n",
      "\n",
      "  File \"/home/v890/v890699/miniconda3/envs/TFM_new/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1150, in train_step\n",
      "\n",
      "  File \"/home/v890/v890699/miniconda3/envs/TFM_new/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n",
      "\n",
      "  File \"/home/v890/v890699/miniconda3/envs/TFM_new/lib/python3.11/site-packages/keras/src/engine/training.py\", line 590, in __call__\n",
      "\n",
      "  File \"/home/v890/v890699/miniconda3/envs/TFM_new/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n",
      "\n",
      "  File \"/home/v890/v890699/miniconda3/envs/TFM_new/lib/python3.11/site-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n",
      "\n",
      "  File \"/home/v890/v890699/miniconda3/envs/TFM_new/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n",
      "\n",
      "  File \"/home/v890/v890699/miniconda3/envs/TFM_new/lib/python3.11/site-packages/keras/src/engine/functional.py\", line 515, in call\n",
      "\n",
      "  File \"/home/v890/v890699/miniconda3/envs/TFM_new/lib/python3.11/site-packages/keras/src/engine/functional.py\", line 672, in _run_internal_graph\n",
      "\n",
      "  File \"/home/v890/v890699/miniconda3/envs/TFM_new/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n",
      "\n",
      "  File \"/home/v890/v890699/miniconda3/envs/TFM_new/lib/python3.11/site-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n",
      "\n",
      "  File \"/home/v890/v890699/miniconda3/envs/TFM_new/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n",
      "\n",
      "  File \"/home/v890/v890699/miniconda3/envs/TFM_new/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py\", line 290, in call\n",
      "\n",
      "  File \"/home/v890/v890699/miniconda3/envs/TFM_new/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py\", line 262, in convolution_op\n",
      "\n",
      "DNN library is not found.\n",
      "\t [[{{node model/conv1d/Conv1D}}]] [Op:__inference_train_function_7397]\n"
     ]
    }
   ],
   "source": [
    "!python TFM_MartaRey/ResNet/main_folds.py resnet 40_1e5_N real_40_1e5_N TFM_MartaRey/ResNet/results_parkinson_real_1106\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
